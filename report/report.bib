@inproceedings{SUMO2018,
          title = {Microscopic Traffic Simulation using SUMO},
         author = {Pablo Alvarez Lopez and Michael Behrisch and Laura Bieker-Walz and Jakob Erdmann and Yun-Pang Fl{\"o}tter{\"o}d and Robert Hilbrich and Leonhard L{\"u}cken and Johannes Rummel and Peter Wagner and Evamarie Wie{\ss}ner},
      publisher = {IEEE},
      booktitle = {The 21st IEEE International Conference on Intelligent Transportation Systems},
           year = {2018},
        journal = {IEEE Intelligent Transportation Systems Conference (ITSC)},
       keywords = {traffic simulation, modelling, optimization},
            url = {https://elib.dlr.de/124092/}
 }

@MISC{road_rule,
author = {},
title = {Ordonnance
sur les règles de la circulation routière - Interdiction de circuler le dimanche et de nuit, Art. 91},
month = {January},
year = {2021},
howpublished={\url{https://fedlex.data.admin.ch/filestore/fedlex.data.admin.ch/eli/cc/1962/1364_1409_1420/20210101/fr/pdf-a/fedlex-data-admin-ch-eli-cc-1962-1364_1409_1420-20210101-fr-pdf-a.pdf}}
}

@article{LSTM,
author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
title = "{Long Short-Term Memory}",
journal = {Neural Computation},
volume = {9},
number = {8},
pages = {1735-1780},
year = {1997},
month = {11},
abstract = "{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}",
issn = {0899-7667},
doi = {10.1162/neco.1997.9.8.1735},
url = {https://doi.org/10.1162/neco.1997.9.8.1735},
eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}




@misc{tensorflow,
    title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
    url={https://www.tensorflow.org/},
    note={Software available from tensorflow.org},
    author={
        Mart\'{\i}n~Abadi and
        Ashish~Agarwal and
        Paul~Barham and
        Eugene~Brevdo and
        Zhifeng~Chen and
        Craig~Citro and
        Greg~S.~Corrado and
        Andy~Davis and
        Jeffrey~Dean and
        Matthieu~Devin and
        Sanjay~Ghemawat and
        Ian~Goodfellow and
        Andrew~Harp and
        Geoffrey~Irving and
        Michael~Isard and
        Yangqing Jia and
        Rafal~Jozefowicz and
        Lukasz~Kaiser and
        Manjunath~Kudlur and
        Josh~Levenberg and
        Dandelion~Man\'{e} and
        Rajat~Monga and
        Sherry~Moore and
        Derek~Murray and
        Chris~Olah and
        Mike~Schuster and
        Jonathon~Shlens and
        Benoit~Steiner and
        Ilya~Sutskever and
        Kunal~Talwar and
        Paul~Tucker and
        Vincent~Vanhoucke and
        Vijay~Vasudevan and
        Fernanda~Vi\'{e}gas and
        Oriol~Vinyals and
        Pete~Warden and
        Martin~Wattenberg and
        Martin~Wicke and
        Yuan~Yu and
        Xiaoqiang~Zheng},
      year={2015},
}

@misc{keras,
  title={Keras},
  author={Chollet, Fran\c{c}ois and others},
  year={2015},
  howpublished={\url{https://keras.io}},
} 

@misc{poisson, title={Simulating events: the Poisson process}, author={Bierlaire, Michel}, year={2012}, url={https://transp-or.epfl.ch/courses/OptSim2012/slides/05b-poisson.pdf}} 

